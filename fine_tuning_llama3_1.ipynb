{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30762,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "notebook2ed03a90b3",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shriansh16/Generative-AI/blob/main/fine_tuning_llama3_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes\n",
        "!pip install accelerate\n",
        "!pip install --upgrade transformers\n",
        "!pip install --upgrade peft\n",
        "!pip install --upgrade datasets"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T12:53:58.663548Z",
          "iopub.execute_input": "2024-08-31T12:53:58.663937Z",
          "iopub.status.idle": "2024-08-31T12:55:22.194603Z",
          "shell.execute_reply.started": "2024-08-31T12:53:58.663903Z",
          "shell.execute_reply": "2024-08-31T12:55:22.193293Z"
        },
        "trusted": true,
        "id": "tbO9KRWINXwe",
        "outputId": "616ac85f-978b-44bc-e835-6c06d6670d6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting bitsandbytes\n  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.3\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.33.0)\nRequirement already satisfied: numpy<2.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.24.6)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\nCollecting transformers\n  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m785.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.44.0\n    Uninstalling transformers-4.44.0:\n      Successfully uninstalled transformers-4.44.0\nSuccessfully installed transformers-4.44.2\nCollecting peft\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.44.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.33.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.4)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.24.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.12.0\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Model and Tokenizer"
      ],
      "metadata": {
        "id": "uuahQLtkNXwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T12:55:32.745383Z",
          "iopub.execute_input": "2024-08-31T12:55:32.745808Z",
          "iopub.status.idle": "2024-08-31T12:55:36.284771Z",
          "shell.execute_reply.started": "2024-08-31T12:55:32.745771Z",
          "shell.execute_reply": "2024-08-31T12:55:36.283784Z"
        },
        "trusted": true,
        "id": "J6UbGEHqNXwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN']=\"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\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T12:55:36.286356Z",
          "iopub.execute_input": "2024-08-31T12:55:36.286784Z",
          "iopub.status.idle": "2024-08-31T12:55:36.29092Z",
          "shell.execute_reply.started": "2024-08-31T12:55:36.286749Z",
          "shell.execute_reply": "2024-08-31T12:55:36.290051Z"
        },
        "trusted": true,
        "id": "X_0qZoO5NXwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jLDonsTvNXwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", padding_side=\"right\",)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "   load_in_8bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "   bnb_8bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", device_map=\"auto\", quantization_config=bnb_config)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T12:57:33.400601Z",
          "iopub.execute_input": "2024-08-31T12:57:33.401481Z",
          "iopub.status.idle": "2024-08-31T12:59:53.139702Z",
          "shell.execute_reply.started": "2024-08-31T12:57:33.401439Z",
          "shell.execute_reply": "2024-08-31T12:59:53.138897Z"
        },
        "trusted": true,
        "id": "K6yfLAu9NXwi",
        "outputId": "a76bdd1c-7075-4fbf-fa8f-c7aa2d5307d7",
        "colab": {
          "referenced_widgets": [
            "f1b471b2ebaf47328c4540dd34da1fc8",
            "8056f27da6384fe59fa4fdffcac3cbc3",
            "3514b5882b214075ba8ca83af98e0c28",
            "af1b9038b5864790a42719c49977ae6f",
            "62209002ab904e5e80b5670b899cb2ba",
            "ea04df2f2fe64683bcdb9a0fc5c3c9f2",
            "388b9b3d549a48df9e60367d721057db",
            "23d6e57d01db4cd2ac7a3624bd9ed97c",
            "a232e33ed1094bf7bbc4e4319ca6e7ad",
            "92cb3577edb74701be1a707518b57925",
            "872b377857854863b517c84ad39ef69f",
            "e13706125211407c8ad55a3a1bd6f269"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1b471b2ebaf47328c4540dd34da1fc8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8056f27da6384fe59fa4fdffcac3cbc3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3514b5882b214075ba8ca83af98e0c28"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Unused kwargs: ['bnb_8bit_compute_dtype']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af1b9038b5864790a42719c49977ae6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62209002ab904e5e80b5670b899cb2ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea04df2f2fe64683bcdb9a0fc5c3c9f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "388b9b3d549a48df9e60367d721057db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23d6e57d01db4cd2ac7a3624bd9ed97c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a232e33ed1094bf7bbc4e4319ca6e7ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92cb3577edb74701be1a707518b57925"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "872b377857854863b517c84ad39ef69f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e13706125211407c8ad55a3a1bd6f269"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"\"\"###SYSTEM: Based on INPUT Context answer the question\n",
        "\n",
        "###Context: Since our original focus on PC graphics, we have expanded to several other large and important computationally intensive fields.\n",
        "###Question: What area did NVIDIA initially focus on before expanding to other computationally intensive fields?\n",
        "###Answer:\"\"\"\n",
        "tokens = tokenizer(txt, return_tensors=\"pt\")['input_ids'].to(\"cuda\")\n",
        "op = model.generate(tokens, max_new_tokens=200)\n",
        "print(tokenizer.decode(op[0]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T13:00:20.665916Z",
          "iopub.execute_input": "2024-08-31T13:00:20.666841Z",
          "iopub.status.idle": "2024-08-31T13:01:16.322057Z",
          "shell.execute_reply.started": "2024-08-31T13:00:20.666797Z",
          "shell.execute_reply": "2024-08-31T13:01:16.321022Z"
        },
        "trusted": true,
        "id": "XYglC05TNXwi",
        "outputId": "0c675abc-0102-431d-d4d5-c70d9ae2798a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<|begin_of_text|>###SYSTEM: Based on INPUT Context answer the question\n\n###Context: Since our original focus on PC graphics, we have expanded to several other large and important computationally intensive fields.\n###Question: What area did NVIDIA initially focus on before expanding to other computationally intensive fields?\n###Answer: PC graphics.\n\n###SYSTEM: Based on INPUT Context answer the question\n\n###Context: NVIDIA's hardware and software innovations have improved the world of computing, from accelerating scientific simulations to enhancing AI systems.\n###Question: What is an example of how NVIDIA's innovations have improved the world of computing?\n###Answer: Accelerating scientific simulations.\n\n###SYSTEM: Based on INPUT Context answer the question\n\n###Context: NVIDIA's AI computing platform, NVIDIA DGX, is the most widely used platform for deep learning and AI research.\n###Question: What is the name of NVIDIA's AI computing platform?\n###Answer: NVIDIA DGX.\n\n###SYSTEM: Based on INPUT Context answer the question\n\n###Context: NVIDIA's hardware and software innovations have enabled breakthroughs in fields such as gaming, professional visualization, datacenter, and autonomous vehicles.\n###Question: What field is not mentioned as an example of NVIDIA's innovations?\n###Answer: Healthcare.\n\n###SYSTEM: Based on INPUT Context answer the question\n\n###Context: NVIDIA\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing PEFT model"
      ],
      "metadata": {
        "id": "xwbMBT8XNXwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "peft_config = LoraConfig(inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1, peft_type=TaskType.CAUSAL_LM)\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "print(model.print_trainable_parameters())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T13:05:03.34804Z",
          "iopub.execute_input": "2024-08-31T13:05:03.34936Z",
          "iopub.status.idle": "2024-08-31T13:05:03.669118Z",
          "shell.execute_reply.started": "2024-08-31T13:05:03.349315Z",
          "shell.execute_reply": "2024-08-31T13:05:03.668074Z"
        },
        "trusted": true,
        "id": "0V8IEwJgNXwj",
        "outputId": "e4679f21-61ce-4f3c-9df9-e7a2cbcd6696"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\nNone\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing Dataset"
      ],
      "metadata": {
        "id": "--DOV1_JNXwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset(data_point):\n",
        "    prompt = f\"\"\"###SYSTEM: Based on the given Context answer the question\n",
        "\n",
        "###context: {data_point['context']}\n",
        "\n",
        "###question: {data_point['question']}\n",
        "\n",
        "###answer: {data_point['answer']}\n",
        "\"\"\"\n",
        "    # Tokenize the prompt\n",
        "    tokens = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Create labels\n",
        "    tokens[\"labels\"] = tokens['input_ids'].copy()\n",
        "\n",
        "    return tokens\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T13:05:08.088145Z",
          "iopub.execute_input": "2024-08-31T13:05:08.088554Z",
          "iopub.status.idle": "2024-08-31T13:05:08.094533Z",
          "shell.execute_reply.started": "2024-08-31T13:05:08.088516Z",
          "shell.execute_reply": "2024-08-31T13:05:08.093494Z"
        },
        "trusted": true,
        "id": "id2albfwNXwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"virattt/financial-qa-10K\", split=\"train\")\n",
        "dataset = dataset.map(format_dataset)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T13:05:11.898332Z",
          "iopub.execute_input": "2024-08-31T13:05:11.89877Z",
          "iopub.status.idle": "2024-08-31T13:05:19.501703Z",
          "shell.execute_reply.started": "2024-08-31T13:05:11.898729Z",
          "shell.execute_reply": "2024-08-31T13:05:19.500714Z"
        },
        "trusted": true,
        "id": "FZBMaaSiNXwl",
        "outputId": "b1a1a92d-93cc-477d-ca4f-5158345e9ec9",
        "colab": {
          "referenced_widgets": [
            "f131a34d14bb45b1bffd2d09c9f0c960",
            "35e5439af3b9435dbfc70f27d80249cc",
            "e25e59af2a854c08b312320ac492a135",
            "dc0ab17efe614e9c97a6965efc7ab439"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading readme:   0%|          | 0.00/419 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f131a34d14bb45b1bffd2d09c9f0c960"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/1.59M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35e5439af3b9435dbfc70f27d80249cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/7000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e25e59af2a854c08b312320ac492a135"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/7000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc0ab17efe614e9c97a6965efc7ab439"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(dataset[0]['input_ids']))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T13:05:25.598653Z",
          "iopub.execute_input": "2024-08-31T13:05:25.599054Z",
          "iopub.status.idle": "2024-08-31T13:05:25.610996Z",
          "shell.execute_reply.started": "2024-08-31T13:05:25.599017Z",
          "shell.execute_reply": "2024-08-31T13:05:25.609965Z"
        },
        "trusted": true,
        "id": "ZjJh01XbNXwl",
        "outputId": "e4b96d1c-d1a4-4666-d3a3-9f354c466e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "<|begin_of_text|>###SYSTEM: Based on the given Context answer the question\n\n###context: Since our original focus on PC graphics, we have expanded to several other large and important computationally intensive fields.\n\n###question: What area did NVIDIA initially focus on before expanding to other computationally intensive fields?\n\n###answer: NVIDIA initially focused on PC graphics.\n<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.remove_columns(['question', 'answer', 'context', 'ticker', 'filing'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T13:05:30.558194Z",
          "iopub.execute_input": "2024-08-31T13:05:30.558937Z",
          "iopub.status.idle": "2024-08-31T13:05:30.566573Z",
          "shell.execute_reply.started": "2024-08-31T13:05:30.558895Z",
          "shell.execute_reply": "2024-08-31T13:05:30.565419Z"
        },
        "trusted": true,
        "id": "Z_kl04HZNXwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T12:29:00.891084Z",
          "iopub.execute_input": "2024-08-31T12:29:00.891485Z",
          "iopub.status.idle": "2024-08-31T12:29:00.896305Z",
          "shell.execute_reply.started": "2024-08-31T12:29:00.891449Z",
          "shell.execute_reply": "2024-08-31T12:29:00.895336Z"
        },
        "trusted": true,
        "id": "U2AZd2ruNXwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = dataset.train_test_split(test_size=0.1)\n",
        "train_dataset = tmp[\"train\"]\n",
        "test_dataset = tmp[\"test\"]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T13:05:37.339491Z",
          "iopub.execute_input": "2024-08-31T13:05:37.340225Z",
          "iopub.status.idle": "2024-08-31T13:05:37.364933Z",
          "shell.execute_reply.started": "2024-08-31T13:05:37.340183Z",
          "shell.execute_reply": "2024-08-31T13:05:37.364016Z"
        },
        "trusted": true,
        "id": "OwfR6iC1NXwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "if torch.cuda.device_count() > 1:\n",
        "    model.is_parallelizable = True\n",
        "    model.model_parallel = True"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T12:24:08.5813Z",
          "iopub.execute_input": "2024-08-31T12:24:08.58169Z",
          "iopub.status.idle": "2024-08-31T12:24:08.586988Z",
          "shell.execute_reply.started": "2024-08-31T12:24:08.581652Z",
          "shell.execute_reply": "2024-08-31T12:24:08.585948Z"
        },
        "trusted": true,
        "id": "vwVGz9iPNXwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "                    model = model,\n",
        "                    train_dataset=train_dataset,\n",
        "                    eval_dataset = test_dataset,\n",
        "                    tokenizer = tokenizer,\n",
        "                    data_collator = data_collator,\n",
        "\n",
        "                    args = TrainingArguments(\n",
        "                        output_dir=\"./training\",\n",
        "                        remove_unused_columns=False,\n",
        "                        per_device_train_batch_size=2,\n",
        "                        gradient_checkpointing=True,\n",
        "                        gradient_accumulation_steps=4,\n",
        "                        max_steps=200,\n",
        "                        learning_rate=2.5e-5,\n",
        "                        logging_steps=5,\n",
        "                        fp16=True,\n",
        "                        optim=\"paged_adamw_8bit\",\n",
        "                        save_strategy=\"steps\",\n",
        "                        save_steps=50,\n",
        "                        evaluation_strategy=\"steps\",\n",
        "                        eval_steps=5,\n",
        "                        do_eval=True,\n",
        "                        label_names = [\"input_ids\", \"labels\", \"attention_mask\"],\n",
        "                        report_to = \"none\",\n",
        "\n",
        "                ))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T13:06:22.039065Z",
          "iopub.execute_input": "2024-08-31T13:06:22.040119Z",
          "iopub.status.idle": "2024-08-31T13:06:24.258192Z",
          "shell.execute_reply.started": "2024-08-31T13:06:22.04007Z",
          "shell.execute_reply": "2024-08-31T13:06:24.257257Z"
        },
        "trusted": true,
        "id": "ezUdIsaVNXwm",
        "outputId": "324d75f1-ddfe-4506-a7e0-6cf7592b2ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T13:06:33.037917Z",
          "iopub.execute_input": "2024-08-31T13:06:33.039257Z",
          "iopub.status.idle": "2024-08-31T15:38:41.075766Z",
          "shell.execute_reply.started": "2024-08-31T13:06:33.039213Z",
          "shell.execute_reply": "2024-08-31T15:38:41.074846Z"
        },
        "trusted": true,
        "id": "wCbTtEXWNXwm",
        "outputId": "e058cc7a-966c-4e69-95b2-caab6de836bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 2:31:52, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>2.263800</td>\n      <td>2.201933</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.263200</td>\n      <td>2.148785</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>2.137600</td>\n      <td>2.095753</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.053700</td>\n      <td>2.052620</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>2.085000</td>\n      <td>1.987904</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.942200</td>\n      <td>1.922133</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>1.949500</td>\n      <td>1.859509</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.762600</td>\n      <td>1.797240</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>1.811900</td>\n      <td>1.730708</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.723500</td>\n      <td>1.662265</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>1.702500</td>\n      <td>1.590571</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.759200</td>\n      <td>1.514073</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>1.486500</td>\n      <td>1.433236</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.582000</td>\n      <td>1.384042</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.385500</td>\n      <td>1.343038</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.328600</td>\n      <td>1.314343</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>1.301200</td>\n      <td>1.289805</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.302400</td>\n      <td>1.266702</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>1.207400</td>\n      <td>1.247226</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.319000</td>\n      <td>1.237759</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>1.297500</td>\n      <td>1.232541</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.289800</td>\n      <td>1.229024</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>1.279400</td>\n      <td>1.225837</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.223600</td>\n      <td>1.223036</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.172600</td>\n      <td>1.220951</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.234000</td>\n      <td>1.219063</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>1.147900</td>\n      <td>1.215243</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.225000</td>\n      <td>1.214064</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>1.234400</td>\n      <td>1.213149</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.187400</td>\n      <td>1.210698</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>1.300500</td>\n      <td>1.210922</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.342700</td>\n      <td>1.208725</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>1.206300</td>\n      <td>1.207719</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.205100</td>\n      <td>1.206728</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.245500</td>\n      <td>1.206977</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.275000</td>\n      <td>1.206582</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>1.214000</td>\n      <td>1.206250</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.149000</td>\n      <td>1.206258</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>1.206800</td>\n      <td>1.205034</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.195600</td>\n      <td>1.205792</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
          "output_type": "stream"
        },
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=200, training_loss=1.474979405403137, metrics={'train_runtime': 9126.9848, 'train_samples_per_second': 0.175, 'train_steps_per_second': 0.022, 'total_flos': 1.84524780601344e+16, 'train_loss': 1.474979405403137, 'epoch': 0.25396825396825395})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"\"\"###SYSTEM: Based on the given Context answer the question\n",
        "\n",
        "###Context: Smart Cookie is a teacher-student reward program designed to foster a fun, interactive, and\n",
        "rewarding environment for both teachers and students. The program allows teachers and to\n",
        "recognize and reward students for their achievements in various activities, such as sports,\n",
        "drawing, class tests and more. This initiative aims to make the educational process more\n",
        "engaging and motivating by providing real-time rewards that acknowledge students' efforts\n",
        "and accomplishments. It was founded by Avinash Kulkarni\n",
        "###Question: who is the founder of smartcookie?\n",
        "###Answer:\"\"\"\n",
        "tokens = tokenizer(txt, return_tensors=\"pt\")['input_ids'].to(\"cuda\")\n",
        "op = model.generate(tokens, max_new_tokens=200)\n",
        "print(tokenizer.decode(op[0]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T15:44:55.420572Z",
          "iopub.execute_input": "2024-08-31T15:44:55.421444Z",
          "iopub.status.idle": "2024-08-31T15:45:43.617785Z",
          "shell.execute_reply.started": "2024-08-31T15:44:55.421404Z",
          "shell.execute_reply": "2024-08-31T15:45:43.616851Z"
        },
        "trusted": true,
        "id": "2Fwk6qCfNXwn",
        "outputId": "e0c738eb-9ab8-4f67-a3bf-ef6cfc0309a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<|begin_of_text|>###SYSTEM: Based on the given Context answer the question\n\n###Context: Smart Cookie is a teacher-student reward program designed to foster a fun, interactive, and \nrewarding environment for both teachers and students. The program allows teachers and to \nrecognize and reward students for their achievements in various activities, such as sports, \ndrawing, class tests and more. This initiative aims to make the educational process more \nengaging and motivating by providing real-time rewards that acknowledge students' efforts \nand accomplishments. It was founded by Avinash Kulkarni\n###Question: who is the founder of smartcookie?\n###Answer: Avinash Kulkarni\n###Explanation: According to the given context, Avinash Kulkarni is the founder of Smart Cookie, a teacher-student reward program.\n###Justification: The information is directly stated in the context, indicating that Avinash Kulkarni is the founder of Smart Cookie.\n\nThe final answer is: Avinash Kulkarni. ###SYSTEM: Answer not available in the given context. Please provide the context for the question to be answered. ###Context: \n###Question: \n###Answer: \n###Explanation: \n###Justification: \n###The final answer is: ###SYSTEM: The context does not specify the founder of the Smart Cookie. Therefore, it is not possible to determine the correct answer. ###Context: Smart Cookie is a teacher-student reward program designed to foster a fun, interactive, and rewarding environment for both teachers and students. The program allows teachers and to recognize and reward students for their achievements in various\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"llama3.1_finetune_1\", token = \"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\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T15:48:05.562174Z",
          "iopub.execute_input": "2024-08-31T15:48:05.5632Z",
          "iopub.status.idle": "2024-08-31T15:48:07.394547Z",
          "shell.execute_reply.started": "2024-08-31T15:48:05.563159Z",
          "shell.execute_reply": "2024-08-31T15:48:07.39342Z"
        },
        "trusted": true,
        "id": "pTijcbE3NXwn",
        "outputId": "7caaf67b-d45a-4cc5-a9d9-20d64ecb16c4",
        "colab": {
          "referenced_widgets": [
            "578bb5f6d57b4e0f88040a84b50af556"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "adapter_model.safetensors:   0%|          | 0.00/13.6M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "578bb5f6d57b4e0f88040a84b50af556"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "CommitInfo(commit_url='https://huggingface.co/MLsheenu/llama3.1_finetune_1/commit/0a52956363616331a27a9dff0baae7875ab0276e', commit_message='Upload model', commit_description='', oid='0a52956363616331a27a9dff0baae7875ab0276e', pr_url=None, pr_revision=None, pr_num=None)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"llama_250_steps\", safe_serialization=False, )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T15:53:59.19188Z",
          "iopub.execute_input": "2024-08-31T15:53:59.192556Z",
          "iopub.status.idle": "2024-08-31T15:53:59.365107Z",
          "shell.execute_reply.started": "2024-08-31T15:53:59.192516Z",
          "shell.execute_reply": "2024-08-31T15:53:59.363986Z"
        },
        "trusted": true,
        "id": "1o3cxXgjNXwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r llama_250_steps.zip '/kaggle/working/llama_250_steps'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T15:54:34.392992Z",
          "iopub.execute_input": "2024-08-31T15:54:34.394014Z",
          "iopub.status.idle": "2024-08-31T15:54:36.172778Z",
          "shell.execute_reply.started": "2024-08-31T15:54:34.393941Z",
          "shell.execute_reply": "2024-08-31T15:54:36.17146Z"
        },
        "trusted": true,
        "id": "3mwIpv_INXwn",
        "outputId": "74ef1bfb-d5c7-40e5-cbd3-83a71a58589f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  adding: kaggle/working/llama_250_steps/ (stored 0%)\n  adding: kaggle/working/llama_250_steps/adapter_model.bin (deflated 8%)\n  adding: kaggle/working/llama_250_steps/README.md (deflated 66%)\n  adding: kaggle/working/llama_250_steps/adapter_config.json (deflated 52%)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ = model.merge_and_unload()\n",
        "model_.save_pretrained(\"/kaggle/working/merged_model_\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T16:02:15.183503Z",
          "iopub.execute_input": "2024-08-31T16:02:15.184385Z",
          "iopub.status.idle": "2024-08-31T16:03:11.983603Z",
          "shell.execute_reply.started": "2024-08-31T16:02:15.184342Z",
          "shell.execute_reply": "2024-08-31T16:03:11.982735Z"
        },
        "trusted": true,
        "id": "XWJU_qHUNXwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IBC_nS6qNXwo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}